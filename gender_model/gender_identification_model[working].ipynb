{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tjj6cFZ02YAg"
      },
      "outputs": [],
      "source": [
        "# @title loading dataset\n",
        "!git clone https://huggingface.co/datasets/sameernotes/indian-gender-identification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title model traning and sample prediction\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load the datasets and print a few examples to verify content\n",
        "female_df = pd.read_csv('/content/indian-gender-identification/Indian-Female-Names.csv')\n",
        "male_df = pd.read_csv('/content/indian-gender-identification/Indian-Male-Names.csv')\n",
        "\n",
        "print(\"Sample female names:\")\n",
        "print(female_df.head())\n",
        "print(\"\\nSample male names:\")\n",
        "print(male_df.head())\n",
        "\n",
        "# Combine the datasets\n",
        "df = pd.concat([female_df, male_df], ignore_index=True)\n",
        "\n",
        "# Check the data\n",
        "print(f\"\\nTotal samples: {len(df)}\")\n",
        "print(f\"Gender distribution:\\n{df['gender'].value_counts()}\")\n",
        "\n",
        "# Clean the data: Convert all names to strings and fill NaN values\n",
        "df['name'] = df['name'].fillna('unknown').astype(str)\n",
        "\n",
        "# Verify class balance\n",
        "print(f\"\\nPercentage of female names: {df[df['gender'] == 'f'].shape[0] / df.shape[0] * 100:.2f}%\")\n",
        "print(f\"Percentage of male names: {df[df['gender'] == 'm'].shape[0] / df.shape[0] * 100:.2f}%\")\n",
        "\n",
        "# Convert gender to numerical labels\n",
        "gender_map = {'f': 0, 'm': 1}\n",
        "df['gender_label'] = df['gender'].map(gender_map)\n",
        "\n",
        "# Create character mapping\n",
        "all_chars = set(''.join(df['name'].str.lower()))\n",
        "char_to_idx = {char: idx+1 for idx, char in enumerate(sorted(all_chars))}\n",
        "char_to_idx['<PAD>'] = 0  # Add padding token\n",
        "\n",
        "# Display character set\n",
        "print(f\"\\nTotal unique characters: {len(all_chars)}\")\n",
        "print(f\"Characters: {''.join(sorted(all_chars))}\")\n",
        "\n",
        "# Get maximum name length to standardize input size\n",
        "max_name_length = df['name'].str.len().max()\n",
        "print(f\"Maximum name length: {max_name_length}\")\n",
        "\n",
        "# Character-level tokenization and padding\n",
        "def tokenize_name(name, max_length=max_name_length):\n",
        "    name = str(name).lower()  # Ensure it's a string\n",
        "    tokens = [char_to_idx[char] if char in char_to_idx else char_to_idx.get(' ', 1) for char in name]\n",
        "\n",
        "    # Pad or truncate to fixed length\n",
        "    if len(tokens) < max_length:\n",
        "        tokens = tokens + [0] * (max_length - len(tokens))\n",
        "    else:\n",
        "        tokens = tokens[:max_length]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Create custom dataset\n",
        "class NameDataset(Dataset):\n",
        "    def __init__(self, names, labels):\n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name_tensor = torch.tensor(tokenize_name(self.names[idx]), dtype=torch.long)\n",
        "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return name_tensor, label_tensor\n",
        "\n",
        "# Improved model architecture with CNN layers\n",
        "class NameGenderClassifierCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_filters=64, filter_sizes=[2, 3, 4], dropout=0.5):\n",
        "        super(NameGenderClassifierCNN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # Convolutional layers with different filter sizes\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(len(filter_sizes) * num_filters, 100)\n",
        "        self.fc2 = nn.Linear(100, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length)\n",
        "\n",
        "        # Embedding layer\n",
        "        x = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "        # Transpose for convolution\n",
        "        x = x.transpose(1, 2)  # (batch_size, embedding_dim, sequence_length)\n",
        "\n",
        "        # Apply convolutions and max-pooling\n",
        "        conv_outputs = []\n",
        "        for conv in self.convs:\n",
        "            conv_out = torch.relu(conv(x))  # (batch_size, num_filters, seq_len - filter_size + 1)\n",
        "            pool_out = torch.max_pool1d(conv_out, conv_out.shape[2])  # (batch_size, num_filters, 1)\n",
        "            conv_outputs.append(pool_out.squeeze(2))  # (batch_size, num_filters)\n",
        "\n",
        "        # Concatenate outputs from different filter sizes\n",
        "        x = torch.cat(conv_outputs, dim=1)  # (batch_size, num_filters * len(filter_sizes))\n",
        "\n",
        "        # Fully connected layers with dropout\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return self.sigmoid(x).squeeze()\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['name'].values,\n",
        "    df['gender_label'].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['gender_label']\n",
        ")\n",
        "\n",
        "# Further split training data to get a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        "    stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = NameDataset(X_train, y_train)\n",
        "val_dataset = NameDataset(X_val, y_val)\n",
        "test_dataset = NameDataset(X_test, y_test)\n",
        "\n",
        "# Smaller batch size for better learning\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Initialize model and move to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Model hyperparameters\n",
        "vocab_size = len(char_to_idx) + 1  # +1 for padding\n",
        "embedding_dim = 100\n",
        "num_filters = 64\n",
        "filter_sizes = [2, 3, 4, 5]  # Multiple filter sizes to capture different n-gram patterns\n",
        "\n",
        "# Initialize the CNN model\n",
        "model = NameGenderClassifierCNN(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_filters=num_filters,\n",
        "    filter_sizes=filter_sizes,\n",
        "    dropout=0.5\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Count model parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total model parameters: {total_params}\")\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "# Lower learning rate for more stable learning\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
        ")\n",
        "\n",
        "# Training function with validation\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=15):\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
        "        for inputs, labels in progress_bar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track statistics\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            predicted = (outputs >= 0.5).float()\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': loss.item(),\n",
        "                'acc': train_correct/train_total\n",
        "            })\n",
        "\n",
        "        # Calculate epoch training statistics\n",
        "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]'):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                predicted = (outputs >= 0.5).float()\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate epoch validation statistics\n",
        "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
        "        epoch_val_acc = val_correct / val_total\n",
        "\n",
        "        # Update learning rate based on validation loss\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if epoch_val_acc > best_val_acc:\n",
        "            best_val_acc = epoch_val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "            print(f\"  --> New best model saved with validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['train_acc'].append(epoch_train_acc)\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print(f'Epoch {epoch+1}/{epochs}:')\n",
        "        print(f'  Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}')\n",
        "        print(f'  Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}')\n",
        "\n",
        "        # Early stopping check\n",
        "        if epoch > 5 and all(history['val_acc'][-i-1] <= history['val_acc'][-i-2] for i in range(3)):\n",
        "            print(\"Early stopping: Validation accuracy hasn't improved for 3 epochs\")\n",
        "            break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "    return history, model\n",
        "\n",
        "# Evaluation function with more metrics\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc='Evaluating'):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probs = outputs.cpu().numpy()\n",
        "            predicted = (outputs >= 0.5).float()\n",
        "\n",
        "            all_probs.extend(probs)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    report = classification_report(all_labels, all_preds, target_names=['Female', 'Male'])\n",
        "\n",
        "    # Calculate threshold metrics\n",
        "    thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "    threshold_metrics = {}\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        thresh_preds = [1 if p >= threshold else 0 for p in all_probs]\n",
        "        thresh_acc = accuracy_score(all_labels, thresh_preds)\n",
        "        threshold_metrics[threshold] = thresh_acc\n",
        "\n",
        "    return accuracy, report, conf_matrix, all_probs, all_labels, threshold_metrics\n",
        "\n",
        "# Print a few examples of names and their expected labels for verification\n",
        "print(\"\\nVerification of dataset:\")\n",
        "for i in range(5):\n",
        "    print(f\"Male example {i+1}: {X_train[y_train == 1][i]}\")\n",
        "    print(f\"Female example {i+1}: {X_train[y_train == 0][i]}\")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nStarting training...\")\n",
        "epochs = 15\n",
        "history, model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\nEvaluating model on test data...\")\n",
        "accuracy, report, conf_matrix, probabilities, true_labels, threshold_metrics = evaluate_model(model, test_loader)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "print(\"\\nAccuracy at different thresholds:\")\n",
        "for threshold, acc in threshold_metrics.items():\n",
        "    print(f\"Threshold {threshold}: {acc:.4f}\")\n",
        "\n",
        "# Plot training and validation history\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(history['train_loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(history['train_acc'], label='Training Accuracy')\n",
        "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "plt.title('Accuracy over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot probability distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "female_probs = [1-p for p, l in zip(probabilities, true_labels) if l == 0]\n",
        "male_probs = [p for p, l in zip(probabilities, true_labels) if l == 1]\n",
        "\n",
        "plt.hist(female_probs, bins=20, alpha=0.5, label='Female names')\n",
        "plt.hist(male_probs, bins=20, alpha=0.5, label='Male names')\n",
        "plt.title('Probability Distribution by Gender')\n",
        "plt.xlabel('Probability of being male')\n",
        "plt.ylabel('Count')\n",
        "plt.legend()\n",
        "plt.savefig('probability_distribution.png')\n",
        "plt.show()\n",
        "\n",
        "# Function to predict gender for new names\n",
        "def predict_gender(name, model, char_to_idx, max_length, threshold=0.5):\n",
        "    model.eval()\n",
        "    tokenized_name = tokenize_name(name, max_length)\n",
        "    input_tensor = torch.tensor([tokenized_name], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        probability = output.item()\n",
        "        predicted_gender = 'Male' if probability >= threshold else 'Female'\n",
        "        confidence = probability if probability >= threshold else 1 - probability\n",
        "\n",
        "    return predicted_gender, probability, confidence\n",
        "\n",
        "# Load the best model for predictions\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "# Example predictions with detailed output\n",
        "test_names = ['Priya', 'Rahul', 'Anjali', 'Vikram', 'Aishwarya', 'Raj', 'Neha', 'Sanjay', 'Pooja']\n",
        "print(\"\\nPredictions on sample names:\")\n",
        "print(\"Name\\t\\tPrediction\\tMale Prob\\tConfidence\")\n",
        "print(\"-\" * 60)\n",
        "for name in test_names:\n",
        "    gender, male_prob, confidence = predict_gender(name, model, char_to_idx, max_name_length)\n",
        "    print(f\"{name:<12}\\t{gender:<10}\\t{male_prob:.4f}\\t\\t{confidence:.4f}\")\n",
        "\n",
        "# Save the model and necessary data\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'char_to_idx': char_to_idx,\n",
        "    'max_name_length': max_name_length,\n",
        "    'model_config': {\n",
        "        'vocab_size': vocab_size,\n",
        "        'embedding_dim': embedding_dim,\n",
        "        'num_filters': num_filters,\n",
        "        'filter_sizes': filter_sizes,\n",
        "    }\n",
        "}, 'indian_name_gender_model.pt')\n",
        "\n",
        "print(\"\\nModel saved to 'indian_name_gender_model.pt'\")\n",
        "\n",
        "# Function to load the model and make predictions\n",
        "def load_model_and_predict(model_path, names):\n",
        "    # Load the saved model\n",
        "    checkpoint = torch.load(model_path)\n",
        "\n",
        "    # Get model configuration and data\n",
        "    char_to_idx = checkpoint['char_to_idx']\n",
        "    max_name_length = checkpoint['max_name_length']\n",
        "    config = checkpoint['model_config']\n",
        "\n",
        "    # Initialize the model\n",
        "    model = NameGenderClassifierCNN(\n",
        "        vocab_size=config['vocab_size'],\n",
        "        embedding_dim=config['embedding_dim'],\n",
        "        num_filters=config['num_filters'],\n",
        "        filter_sizes=config['filter_sizes']\n",
        "    )\n",
        "\n",
        "    # Load the state dictionary\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "\n",
        "    # Make predictions\n",
        "    results = []\n",
        "    for name in names:\n",
        "        gender, male_prob, confidence = predict_gender(name, model, char_to_idx, max_name_length)\n",
        "        results.append({\n",
        "            'name': name,\n",
        "            'predicted_gender': gender,\n",
        "            'male_probability': male_prob,\n",
        "            'confidence': confidence\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example of using the loading function\n",
        "print(\"\\nExample of loading the model and making predictions:\")\n",
        "sample_names = ['Deepika', 'Arjun', 'Meera', 'Rajesh']\n",
        "predictions = load_model_and_predict('indian_name_gender_model.pt', sample_names)\n",
        "for pred in predictions:\n",
        "    print(f\"{pred['name']}: {pred['predicted_gender']} (male prob: {pred['male_probability']:.4f}, confidence: {pred['confidence']:.4f})\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zAOY9LDR55Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title testing the model\n",
        "print(\"\\nExample of loading the model and making predictions:\")\n",
        "sample_names = ['Deepika', 'sakshi', 'Meera', 'sameer']\n",
        "predictions = load_model_and_predict('indian_name_gender_model.pt', sample_names)\n",
        "for pred in predictions:\n",
        "    print(f\"{pred['name']}: {pred['predicted_gender']} (male prob: {pred['male_probability']:.4f}, confidence: {pred['confidence']:.4f})\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wmqmSuKq8pqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}